# AI Media Analysis

A modular, fully open-source AI pipeline for automated analysis of videos and image series. Designed for sensitive content filtering, structured metadata generation, and human-in-the-loop refinement.

## ðŸš€ Features

- ðŸ”Ž Person detection, tracking & ReID (face and body)
- ðŸ§  Action recognition, emotion detection, pose estimation
- ðŸ‘• Clothing & body description (optional)
- ðŸŽ™ Speech-to-text transcription (Whisper), including translation
- ðŸš« NSFW detection and content classification
- ðŸ§¾ OCR for watermark/logo/title extraction
- ðŸ§ Manual object labeling with review UI
- ðŸ“ Supports both videos and sequential image folders
- ðŸ“Š LLM-based summarization & tagging
- ðŸ§  Continual learning with manual feedback
- ðŸ§© Fully modular via Docker microservices
- ðŸ§˜ On-demand GPU usage via Vast.ai or RunPod (autoscaling)
- ðŸ§± Minimal base system (Netcup VPS) for orchestration and UI

## ðŸ“¦ Architecture Overview

```mermaid
flowchart LR
    A[Media Input] --> B[Keyframe Sampling / Sorting]
    B --> C[Vision Services (Dockerized)]
    C --> D[Metadata Collector]
    D --> E[Vector DB (Qdrant)]
    D --> F[chunk-meta.json]
    F --> G[LLM Summarization (API)]
    G --> H[Safety Check â†’ Fallback LLM if needed]
    H --> I[Streamlit Review UI]

ðŸ—‚ Directory Structure

ai_media_analysis/
â”œâ”€â”€ .env.example              # Environment variable template
â”œâ”€â”€ docker-compose.yml        # Docker stack for media services
â”œâ”€â”€ setup/                    # Deployment and folder setup scripts
â”œâ”€â”€ services/                 # Modular microservices
â”‚   â”œâ”€â”€ control/              # Scheduler, Watchdog
â”‚   â”œâ”€â”€ ui/                   # Streamlit review interface
â”‚   â”œâ”€â”€ vector_db/            # Qdrant vector database config
â”‚   â”œâ”€â”€ object_review/        # Manual labeling logic
â”‚   â””â”€â”€ vision_pipeline/      # Detection, embedding, NSFW, etc.
â”œâ”€â”€ data_schema/              # JSON schema definitions
â””â”€â”€ docs/                     # Architecture notes and instructions
âš™ Requirements

    Host: Linux VPS with at least 4 vCPU, 8 GB RAM, 100+ GB SSD

    Python â‰¥ 3.10

    Docker + Docker Compose

    Optional GPU Node via Vast.ai / RunPod

    Optional: OpenInterpreter for AI-assisted deployment

ðŸ§ª Status

This project is under active development. First prototype deployment scripts and LLM prompt logic are in progress.
ðŸ“œ License

MIT â€” All components are fully open-source. Only optional LLM APIs (e.g. Gemini, Claude, Llama-3) may involve costs.



## ðŸ§  AI Architecture

The pipeline follows a modular, GPU-on-demand structure:

1. **Ingest:** Scene detection and frame extraction
2. **Vision Pipeline:** Person detection, ReID, action/emotion/pose recognition, OCR for titles/logos
3. **Vector Storage & Metadata:** Qdrant for embeddings, JSONs for structured metadata
4. **LLM Layer:** Gemini / OpenAI / Claude via OpenRouter for summarization & safety-checks
5. **UI Tools:** Streamlit-based interface for review, labeling and feedback

All services are containerized. GPU nodes are activated on demand using control logic from the `control` module.
