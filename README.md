# AI Media Analysis

**Version:** 0.4.0 (31. Mai 2025)

A modular, fully open-source AI pipeline for automated analysis of videos and image series. Designed for sensitive content filtering, structured metadata generation, and human-in-the-loop refinement.

## ðŸš€ Features

- ðŸ”Ž Person detection, tracking & ReID (face and body)
- ðŸ§  Action recognition, emotion detection, pose estimation
- ðŸ‘• Clothing & body description (optional)
- ðŸŽ™ Speech-to-text transcription (Whisper), including translation
- ðŸš« NSFW detection and content classification
- ðŸ§¾ OCR for watermark/logo/title extraction
- ðŸ§ Manual object labeling with review UI
- ðŸ“ Supports both videos and sequential image folders
- ðŸ“Š LLM-based summarization & tagging
- ðŸ§  Continual learning with manual feedback
- ðŸ§© Fully modular via Docker microservices
- ðŸ§˜ On-demand GPU usage via Vast.ai or RunPod (autoscaling)
- ðŸ§± Minimal base system (Netcup VPS) for orchestration and UI

## ðŸ“¦ Architecture Overview

```mermaid
flowchart LR
    A[Media Input] --> B[Keyframe Sampling / Sorting]
    B --> C[Vision Services (Dockerized)]
    C --> D[Metadata Collector]
    D --> E[Vector DB (Qdrant)]
    D --> F[chunk-meta.json]
    F --> G[LLM Summarization (API)]
    G --> H[Safety Check â†’ Fallback LLM if needed]
    H --> I[Streamlit Review UI]
```

## ðŸ—‚ Directory Structure

ai_media_analysis/
â”œâ”€â”€ .env.example              # Environment variable template
â”œâ”€â”€ docker-compose.yml        # Docker stack for media services
â”œâ”€â”€ setup/                    # Deployment and folder setup scripts
â”œâ”€â”€ services/                 # Modular microservices
â”‚   â”œâ”€â”€ control/              # Scheduler, Watchdog
â”‚   â”œâ”€â”€ ui/                   # Streamlit review interface
â”‚   â”œâ”€â”€ vector_db/            # Qdrant vector database config
â”‚   â”œâ”€â”€ object_review/        # Manual labeling logic
â”‚   â”œâ”€â”€ pose_estimation/      # Pose detection and analysis
â”‚   â””â”€â”€ vision_pipeline/      # Detection, embedding, NSFW, etc.
â”œâ”€â”€ data_schema/              # JSON schema definitions
â””â”€â”€ docs/                     # Architecture notes and instructions

## âš™ Requirements

    Host: Linux VPS with at least 4 vCPU, 8 GB RAM, 100+ GB SSD
    Python â‰¥ 3.10
    Docker + Docker Compose
    Optional GPU Node via Vast.ai / RunPod
    Optional: OpenInterpreter for AI-assisted deployment

## ðŸ§ª Status

This project is under active development. Current focus is on implementing and integrating AI modules, starting with Pose Estimation.

## ðŸ“œ License

MIT â€” All components are fully open-source. Only optional LLM APIs (e.g. Gemini, Claude, Llama-3) may involve costs.

## ðŸ§  AI Architecture

The pipeline follows a modular, GPU-on-demand structure:

- **Ingest Module:** Scene detection and key-frame extraction (FFmpeg, PySceneDetect)
- **Vision Pipeline:** 
  - YOLOv9 for object detection
  - DeepSORT for tracking
  - Face and body ReID (SCRFD, ArcFace, OSNet)
  - Pose estimation (PARE), action recognition (MMAction2), emotion detection (DeepFace)
  - OCR for logos/titles, NSFW classification
  - Whisper for speech-to-text
- **Storage:**
  - Qdrant for vector embeddings
  - Metadata stored in structured JSON files
- **LLM Processing:**
  - Summarization and safety-checks via Gemini, OpenAI, or Claude (via OpenRouter)
- **Manual Review:**
  - Streamlit-based UI for data inspection, merging, feedback, and training

Each component runs in its own Docker container, orchestrated from a central control node.

## Job-Management

Das System unterstÃ¼tzt sowohl automatische als auch manuelle Job-Verarbeitung. StandardmÃ¤ÃŸig ist die automatische Verarbeitung deaktiviert, sodass Jobs nur manuell durch den Operator gestartet werden kÃ¶nnen.

### API-Endpunkte

Der Job-Manager bietet folgende API-Endpunkte:

#### Batch-Erstellung
```http
POST /batches
{
  "job_ids": ["job1", "job2", "job3"],
  "batch_name": "mein_batch"
}
```

#### Batch-Start
```http
POST /batches/{batch_id}/start
```

#### Batch-Informationen
```http
GET /batches/{batch_id}
```

#### Batch-Liste
```http
GET /batches
```

#### Wartende Jobs
```http
GET /jobs/pending
```

#### Auto-Process-Einstellung
```http
GET /settings/auto-process
POST /settings/auto-process?enabled=true
```

### Umgebungsvariablen

Die folgenden Umgebungsvariablen kÃ¶nnen konfiguriert werden:

- `REDIS_HOST`: Redis-Host (Standard: redis)
- `REDIS_PORT`: Redis-Port (Standard: 6379)
- `BATCH_THRESHOLD_HOURS`: Maximale Batch-Dauer in Stunden (Standard: 4)
- `MIN_JOBS_PER_BATCH`: Minimale Anzahl von Jobs pro Batch (Standard: 3)
- `AUTO_PROCESS_JOBS`: Automatische Job-Verarbeitung aktivieren (Standard: false)
- `VAST_API_KEY`: API-Key fÃ¼r Vast.ai
- `RUNPOD_API_KEY`: API-Key fÃ¼r RunPod

### Installation

1. Repository klonen
2. Umgebungsvariablen konfigurieren (siehe `.env.example`)
3. Docker Compose starten:
   ```bash
   docker-compose up -d
   ```

### Verwendung

1. Medien in das `data/incoming`-Verzeichnis hochladen
2. Jobs werden automatisch erkannt und in der Warteschlange gespeichert
3. Ãœber die API kÃ¶nnen Batches erstellt und gestartet werden
4. GPU-Instanzen werden automatisch erstellt und nach Abschluss gelÃ¶scht

### GPU-Provider

Das System unterstÃ¼tzt derzeit folgende GPU-Provider:

- Vast.ai
- RunPod (in Entwicklung)

### Batch-Verarbeitung

- Jobs werden nach Typ (Video/Bild) gruppiert
- Batches werden optimiert nach geschÃ¤tzter Verarbeitungsdauer
- GPU-Instanzen werden automatisch erstellt und verwaltet
- Nach Abschluss werden alle temporÃ¤ren Dateien bereinigt

### Fehlerbehandlung

- Automatische Wiederholungsversuche bei GPU-Fehlern
- Detaillierte Logging-Informationen
- Status-Updates Ã¼ber die API
