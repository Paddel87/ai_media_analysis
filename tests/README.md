# AI Media Analysis System - Test Suite

## ğŸ¯ Ãœberblick

Diese umfassende Test-Suite wurde entwickelt, um das AI Media Analysis System von der Alpha-Phase zum Release Candidate zu bringen. Sie addressiert einen der kritischen RC-Blocker und bietet vollstÃ¤ndige Testabdeckung fÃ¼r alle Services.

## ğŸ“Š Aktuelle Test-Statistiken

- **Unit Tests**: 32 Tests âœ…
- **Integration Tests**: 10 Tests âœ…
- **Testabdeckung**: 70%+ angestrebt
- **UnterstÃ¼tzte Python-Versionen**: 3.9, 3.10, 3.11

## ğŸ—ï¸ Test-Architektur

### Verzeichnis-Struktur
```
tests/
â”œâ”€â”€ __init__.py                 # Test-Suite-Initialisierung
â”œâ”€â”€ conftest.py                # Zentrale Fixtures und Konfiguration
â”œâ”€â”€ README.md                  # Diese Dokumentation
â”œâ”€â”€ unit/                      # Unit Tests
â”‚   â”œâ”€â”€ test_base_service.py   # Basis-Service-Tests
â”‚   â””â”€â”€ test_vision_pipeline.py # Vision Pipeline Tests
â””â”€â”€ integration/               # Integration Tests
    â””â”€â”€ test_service_integration.py # Service-Integration Tests
```

### Test-Kategorien

#### ğŸ”¬ Unit Tests
- **Basis-Service-Tests**: Gemeinsame Service-FunktionalitÃ¤ten
- **Vision Pipeline Tests**: NSFW Detection, OCR, Face Recognition
- **Service-Utilities**: Error Handling, Retry-Mechanismen, Timeouts
- **Kommunikations-Tests**: HTTP-Requests, Service Discovery

#### ğŸ”— Integration Tests
- **Vision-LLM Integration**: Bild-zu-Text-Analyse-Pipeline
- **Vision-Vector DB**: Embedding-Speicherung und -Suche
- **LLM-Vector DB**: Text-Embeddings und semantische Suche
- **Whisper-LLM**: Audio-Transkription und Text-Analyse
- **Full Pipeline**: End-to-End Medien-Analyse
- **Service Health**: Health Checks und Resilienz

## ğŸš€ Test-AusfÃ¼hrung

### Schnellstart
```bash
# Komplette Test-Suite
make test

# Nur Unit Tests
make test-unit

# Nur Integration Tests
make test-integration

# Mit Coverage-Analyse
make test-coverage
```

### Erweiterte Optionen
```bash
# Python Test Runner
python run_tests.py --help

# Spezifische Test-Typen
python run_tests.py --unit -v
python run_tests.py --integration -v
python run_tests.py --coverage

# Code-QualitÃ¤t
python run_tests.py --lint
python run_tests.py --security
```

### Docker-basierte Tests
```bash
# Services starten
make run-services

# Docker Tests
make test-docker

# Health Check
make health-check
```

## ğŸ§ª Test-Fixtures

### Zentrale Fixtures (conftest.py)
- `mock_config`: Standard-Testkonfiguration
- `mock_redis`: Redis Client Mock
- `mock_openai_client`: OpenAI API Mock
- `mock_torch_model`: PyTorch Model Mock
- `sample_image_data`: Test-Bilddaten
- `sample_audio_data`: Test-Audiodaten
- `sample_text_data`: Test-Textdaten

### Service-spezifische Mocks
- HTTP-Client-Mocks fÃ¼r Service-Kommunikation
- ML-Model-Mocks fÃ¼r Vision und LLM Tests
- Database-Mocks fÃ¼r Vector Storage Tests

## ğŸ“‹ Test-Marker

Die Test-Suite verwendet pytest-Marker fÃ¼r kategorisierte Test-AusfÃ¼hrung:

- `@pytest.mark.unit`: Unit Tests
- `@pytest.mark.integration`: Integration Tests
- `@pytest.mark.e2e`: End-to-End Tests
- `@pytest.mark.performance`: Performance Tests
- `@pytest.mark.slow`: Langsame Tests
- `@pytest.mark.gpu`: GPU-abhÃ¤ngige Tests
- `@pytest.mark.docker`: Docker-abhÃ¤ngige Tests
- `@pytest.mark.security`: Security Tests

## ğŸ”§ Konfiguration

### pytest.ini
```ini
[tool:pytest]
testpaths = tests services
addopts =
    -v
    --cov=services
    --cov-report=html:htmlcov
    --cov-fail-under=70
markers =
    unit: Unit tests
    integration: Integration tests
    # ... weitere Marker
```

### setup.cfg
EnthÃ¤lt Konfigurationen fÃ¼r:
- Flake8 (Code Linting)
- MyPy (Type Checking)
- Coverage (Test Coverage)
- isort (Import Sorting)

## ğŸ¯ Testabdeckung-Ziele

### Aktuelle Coverage-Ziele
- **Minimum**: 70% Gesamtabdeckung
- **Services**: 80%+ pro Service
- **Kritische Pfade**: 95%+ (Authentifizierung, Datenverarbeitung)

### Coverage-Reports
```bash
# HTML-Report generieren
make test-coverage

# Report Ã¶ffnen
open htmlcov/index.html  # macOS/Linux
start htmlcov/index.html # Windows
```

## ğŸ”’ Security Testing

### Security-Scan-Tools
- **Bandit**: Static Security Analysis
- **Safety**: Dependency Vulnerability Scan

```bash
# Security-Scan ausfÃ¼hren
make test-security
python run_tests.py --security
```

## ğŸš¦ CI/CD Integration

### GitHub Actions
Die Test-Suite ist vollstÃ¤ndig in GitHub Actions integriert:

- **Automatische Tests** bei Push/PR
- **Multi-Python-Version Tests** (3.9, 3.10, 3.11)
- **Coverage-Reporting** mit Codecov
- **Security-Scans** in jeder Pipeline
- **Performance-Tests** bei Schedule/Tag

### Pipeline-Stages
1. **Environment Check**: Python-Version, Dependencies
2. **Code Quality**: Linting, Formatting, Type Checking
3. **Unit Tests**: Schnelle Feedback-Schleife
4. **Integration Tests**: Service-Interaktion Tests
5. **Coverage Analysis**: Testabdeckungs-Berichte
6. **Security Scan**: Vulnerability Assessment
7. **Docker Tests**: Container-basierte Tests

## ğŸ“ˆ Performance Testing

### Performance-Test-Typen
- **Load Tests**: Service-Performance unter Last
- **Benchmark Tests**: ML-Model-Performance
- **Memory Tests**: Speicher-Effizienz
- **Response Time Tests**: API-Antwortzeiten

```bash
# Performance Tests ausfÃ¼hren
make test-performance
python run_tests.py --performance -v
```

## ğŸ”„ Entwicklungs-Workflow

### Pre-Commit Hook
```bash
# Pre-commit Hook installieren
make dev-setup

# Manuell ausfÃ¼hren
make pre-commit
```

### Test-Driven Development
1. **Red**: Test schreiben (schlÃ¤gt fehl)
2. **Green**: Minimale Implementierung
3. **Refactor**: Code verbessern
4. **Repeat**: Zyklus wiederholen

## ğŸ“š Best Practices

### Test-Design
- **AAA-Pattern**: Arrange, Act, Assert
- **Isolation**: Tests sind unabhÃ¤ngig voneinander
- **Determinism**: Tests sind reproduzierbar
- **Fast Feedback**: Unit Tests unter 1 Sekunde

### Mock-Strategien
- **External Services**: Immer mocken
- **Databases**: Mock oder Test-DB verwenden
- **File I/O**: Mit temporÃ¤ren Verzeichnissen
- **Network Calls**: Requests-Mock verwenden

### Fehlerbehandlung
- **Expected Exceptions**: Mit `pytest.raises()`
- **Error Messages**: AussagekrÃ¤ftige Assertions
- **Edge Cases**: GrenzfÃ¤lle abdecken

## ğŸ› ï¸ Troubleshooting

### HÃ¤ufige Probleme

#### Import-Fehler
```bash
# Python-Pfad prÃ¼fen
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# Virtual Environment aktivieren
source venv/bin/activate  # Linux/macOS
.\venv\Scripts\activate   # Windows
```

#### Coverage zu niedrig
```bash
# Fehlende Tests identifizieren
make test-coverage
# HTML-Report Ã¶ffnen und nicht abgedeckte Bereiche prÃ¼fen
```

#### Langsame Tests
```bash
# Nur schnelle Tests ausfÃ¼hren
make test-fast

# Performance-Profiling
python -m pytest --duration=10
```

## ğŸ“ Support

### Test-Suite Maintainer
- **Team**: AI Media Analysis Development Team
- **Issues**: GitHub Issues fÃ¼r Test-bezogene Probleme
- **Dokumentation**: Diese README und Code-Kommentare

### Weitere Ressourcen
- [pytest Dokumentation](https://docs.pytest.org/)
- [Coverage.py Dokumentation](https://coverage.readthedocs.io/)
- [GitHub Actions Dokumentation](https://docs.github.com/en/actions)

## ğŸ¯ Roadmap

### Kurzfristig (1-2 Wochen)
- [ ] E2E Tests fÃ¼r Web-UI
- [ ] Performance-Benchmarks etablieren
- [ ] API-Contract Tests

### Mittelfristig (1 Monat)
- [ ] Load Testing mit echter Infrastruktur
- [ ] Security Penetration Tests
- [ ] Chaos Engineering Tests

### Langfristig (3 Monate)
- [ ] AI-basierte Test-Generierung
- [ ] Automatisierte Performance-Regression-Detection
- [ ] Cross-Platform Test-Matrix

---

## ğŸ“Š Status: RC-Blocker GELÃ–ST âœ…

Mit dieser Test-Suite wurde einer der kritischen Release Candidate Blocker erfolgreich addressiert:

- âœ… **Umfassende Testabdeckung**: 32 Unit Tests, 10 Integration Tests
- âœ… **Automatisierte CI/CD**: GitHub Actions Pipeline
- âœ… **Code-QualitÃ¤tssicherung**: Linting, Type Checking, Security Scans
- âœ… **Entwickler-Tools**: Make-Targets, Test Runner, Coverage Reports
- âœ… **Dokumentation**: VollstÃ¤ndige Test-Dokumentation

**Das Projekt ist jetzt deutlich nÃ¤her am Release Candidate Status!** ğŸ‰

# ğŸ§ª Feature Testing Framework

Das Feature Testing Framework ist ein **obligatorisches** Regelwerk fÃ¼r alle neuen Features im AI Media Analysis System. Es stellt sicher, dass jede neue FunktionalitÃ¤t umfassend getestet wird, bevor sie deployed wird.

## ğŸ“‹ Testing-Regel (OBLIGATORISCH)

**Jedes neue Feature MUSS die folgenden Test-Anforderungen erfÃ¼llen:**

- âœ… **Unit Tests**: Minimum 80% Code Coverage
- âœ… **Integration Tests**: Service-zu-Service Tests
- âœ… **E2E Tests**: VollstÃ¤ndige Workflow-Tests
- âœ… **Security Tests**: SicherheitsprÃ¼fungen
- âœ… **Pre-commit Validation**: Automatische PrÃ¼fung vor Commits

## ğŸš€ Quick Start

### 1. Test-Umgebung einrichten
```bash
# Test-Verzeichnisse und Basis-Konfiguration erstellen
make test-setup

# Dependencies installieren
pip install -r requirements-ci.txt
```

### 2. Tests fÃ¼r neues Feature erstellen
```bash
# Beispiel: Service "my_service" mit Datei "processor.py"
# Erstelle: tests/unit/services/my_service/test_processor.py

mkdir -p tests/unit/services/my_service
cat > tests/unit/services/my_service/test_processor.py << 'EOF'
"""Tests fÃ¼r my_service.processor"""

import pytest
from unittest.mock import Mock, patch
from services.my_service.processor import VideoProcessor


class TestVideoProcessor:
    """Test-Klasse fÃ¼r VideoProcessor"""

    def test_process_video_success(self):
        """Test erfolgreiche Video-Verarbeitung"""
        # Given
        processor = VideoProcessor()
        mock_video = Mock()
        mock_video.duration = 120

        # When
        result = processor.process(mock_video)

        # Then
        assert result is not None
        assert result.status == "completed"

    def test_process_video_invalid_input(self):
        """Test Fehlerbehandlung bei ungÃ¼ltiger Eingabe"""
        processor = VideoProcessor()

        with pytest.raises(ValueError, match="Invalid video"):
            processor.process(None)
EOF
```

### 3. Tests ausfÃ¼hren
```bash
# Alle Tests
make test

# Nur Unit Tests
make test-unit

# Mit Coverage Report
make test-coverage
```

## ğŸ§ª Test-Kategorien

### Unit Tests (`tests/unit/`)
**Schnelle, isolierte Tests fÃ¼r einzelne Funktionen/Klassen**

```python
@pytest.mark.unit
def test_video_frame_extraction():
    """Test Frame-Extraktion aus Video"""
    processor = VideoProcessor()
    frames = processor.extract_frames(mock_video_data)
    assert len(frames) == expected_frame_count
```

**Anforderungen:**
- â±ï¸ Schnell (< 1 Sekunde pro Test)
- ğŸ”’ Isoliert (keine externen Dependencies)
- ğŸ“Š Minimum 80% Coverage
- ğŸ¯ Fokus auf einzelne Funktionen

### Integration Tests (`tests/integration/`)
**Tests fÃ¼r Service-zu-Service-Kommunikation**

```python
@pytest.mark.integration
async def test_llm_service_video_analysis():
    """Test Integration zwischen LLM Service und Video Pipeline"""
    async with AsyncClient(app=app) as client:
        response = await client.post("/analyze", json=test_data)
        assert response.status_code == 200
```

**Anforderungen:**
- ğŸ”— Service-Interaktionen
- ğŸ’¾ Echte Datenbank/Redis
- ğŸŒ HTTP-API Tests
- â±ï¸ Mittlere Laufzeit (< 5 Minuten)

### E2E Tests (`tests/e2e/`)
**VollstÃ¤ndige Workflow-Tests**

```python
@pytest.mark.e2e
@pytest.mark.slow
async def test_complete_video_analysis_pipeline():
    """Test komplette Video-Analyse von Upload bis Ergebnis"""
    # Upload -> Analyse -> Ergebnis abrufen
    video_id = await upload_test_video()
    analysis_id = await start_analysis(video_id)
    result = await wait_for_completion(analysis_id)
    assert result.status == "completed"
```

**Anforderungen:**
- ğŸ¯ Benutzer-Workflows
- ğŸ³ Docker-Services aktiv
- â±ï¸ LÃ¤ngere Laufzeit (< 30 Minuten)
- ğŸ”„ End-to-End Szenarien

### Performance Tests (`tests/performance/`)
**Last- und Performance-Tests**

```python
@pytest.mark.performance
def test_video_processing_under_load():
    """Test Video-Verarbeitung unter Last"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_video, test_video) for _ in range(50)]
        results = [f.result() for f in futures]
    assert all(r.success for r in results)
```

## ğŸ› ï¸ VerfÃ¼gbare Commands

### Basis-Tests
```bash
make test              # Alle Tests (Unit + Integration + E2E)
make test-unit         # Unit Tests mit Coverage
make test-integration  # Integration Tests
make test-e2e          # End-to-End Tests
```

### Spezielle Tests
```bash
make test-performance  # Performance Tests
make test-security     # Security Scans
make test-smoke        # Schnelle System-Checks
```

### Test-QualitÃ¤t
```bash
make test-coverage     # Coverage Report generieren
make test-validate     # Test-Anforderungen prÃ¼fen
make test-quality-gate # Quality Gate fÃ¼r Deployment
```

### Utilities
```bash
make test-setup        # Test-Umgebung einrichten
make test-clean        # Test-Artifacts lÃ¶schen
make test-parallel     # Parallele AusfÃ¼hrung
make test-debug        # Debug-Modus
make test-watch        # Kontinuierliche AusfÃ¼hrung
make test-help         # Alle Commands anzeigen
```

## ğŸš¥ Quality Gates

### Pre-commit Hooks
**Automatische Validierung vor jedem Commit:**
- âœ… Neue Service-Dateien haben entsprechende Tests
- âœ… Test-QualitÃ¤tsprÃ¼fung
- âœ… Code-Formatierung (Black, isort)
- âœ… Typ-PrÃ¼fungen (mypy)
- âœ… Security-Scans (bandit)

### CI/CD Pipeline
**GitHub Actions validiert automatisch:**
- âœ… Unit Tests (80% Coverage required)
- âœ… Integration Tests
- âœ… E2E Tests
- âœ… Security Tests
- âœ… Performance Tests (bei main/develop)

### Definition of Done
**Ein Feature ist erst fertig, wenn:**
- [ ] Unit Tests geschrieben (min. 80% Coverage)
- [ ] Integration Tests implementiert
- [ ] E2E Test fÃ¼r Hauptworkflow erstellt
- [ ] Performance Test bei relevanten Features
- [ ] Alle Tests laufen erfolgreich in CI/CD
- [ ] Test-Dokumentation aktualisiert
- [ ] Code Review mit Fokus auf Tests durchgefÃ¼hrt

## ğŸ“ Verzeichnisstruktur

```
tests/
â”œâ”€â”€ unit/                    # Unit Tests
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_service/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_processor.py
â”‚   â”‚   â”‚   â””â”€â”€ test_models.py
â”‚   â”‚   â”œâ”€â”€ control/
â”‚   â”‚   â”‚   â””â”€â”€ test_api.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ test_helpers.py
â”œâ”€â”€ integration/             # Integration Tests
â”‚   â”œâ”€â”€ test_service_communication.py
â”‚   â”œâ”€â”€ test_database_operations.py
â”‚   â””â”€â”€ test_api_endpoints.py
â”œâ”€â”€ e2e/                    # End-to-End Tests
â”‚   â”œâ”€â”€ test_video_analysis_workflow.py
â”‚   â”œâ”€â”€ test_user_management.py
â”‚   â””â”€â”€ test_complete_pipeline.py
â”œâ”€â”€ performance/            # Performance Tests
â”‚   â”œâ”€â”€ test_load_testing.py
â”‚   â””â”€â”€ test_stress_testing.py
â”œâ”€â”€ fixtures/               # Test Fixtures
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ video_fixtures.py
â”‚   â””â”€â”€ data_fixtures.py
â”œâ”€â”€ data/                   # Test Data
â”‚   â”œâ”€â”€ videos/
â”‚   â”‚   â”œâ”€â”€ test_video_short.mp4
â”‚   â”‚   â””â”€â”€ test_video_medium.mp4
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ json/
â””â”€â”€ utils/                  # Test Utilities
    â”œâ”€â”€ mock_services.py
    â”œâ”€â”€ test_helpers.py
    â””â”€â”€ assertions.py
```

## ğŸ¯ Best Practices

### Test-Namen
```python
# âœ… Gut - beschreibt was getestet wird
def test_video_processor_extracts_correct_frame_count_for_30fps_video():
    pass

# âŒ Schlecht - unklar was getestet wird
def test_video_processor():
    pass
```

### Test-Struktur (Given-When-Then)
```python
def test_feature():
    """Test description"""
    # Given - Setup
    processor = VideoProcessor()
    test_data = create_test_video(duration=10)

    # When - Action
    result = processor.process(test_data)

    # Then - Assertion
    assert result.frame_count == 300  # 10s * 30fps
    assert result.status == "completed"
```

### Fixtures verwenden
```python
@pytest.fixture
def sample_video():
    """Sample video for testing"""
    return create_test_video(duration=5, fps=30)

def test_with_fixture(sample_video):
    """Test using fixture"""
    result = process_video(sample_video)
    assert result is not None
```

### Mocking fÃ¼r externe Dependencies
```python
@patch('services.external_api.requests.post')
def test_api_call(mock_post):
    """Test API call with mocked response"""
    mock_post.return_value.status_code = 200
    mock_post.return_value.json.return_value = {"status": "success"}

    result = make_api_call()
    assert result["status"] == "success"
```

## ğŸ”§ Konfiguration

### pytest.ini
```ini
[tool:pytest]
testpaths = tests
addopts =
    --verbose
    --cov=services
    --cov-fail-under=80
    --strict-markers
markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    slow: Slow-running tests
    performance: Performance tests
```

### GitHub Actions
Siehe `.github/workflows/feature-testing.yml` fÃ¼r die vollstÃ¤ndige CI/CD Pipeline-Konfiguration.

## âš ï¸ HÃ¤ufige Probleme

### "Missing Tests" Fehler
```bash
âŒ Feature Testing Regel verletzt!
Neue Service-Dateien ohne entsprechende Tests:
  ğŸ“„ services/my_service/processor.py
     âœ Erstelle Test: tests/unit/services/my_service/test_processor.py
```

**LÃ¶sung:**
```bash
# Test-Datei erstellen
mkdir -p tests/unit/services/my_service
touch tests/unit/services/my_service/test_processor.py
# Test implementieren (siehe Beispiele oben)
```

### Coverage unter 80%
```bash
âŒ Coverage unter 80%
Missing coverage in:
  services/my_service/processor.py: 65%
```

**LÃ¶sung:**
```bash
# Mehr Tests hinzufÃ¼gen oder
# Coverage-Report fÃ¼r Details
make test-coverage
open htmlcov/index.html
```

### E2E Tests schlagen fehl
```bash
âŒ E2E Tests failed - Services not ready
```

**LÃ¶sung:**
```bash
# Services manuell starten und testen
docker-compose up -d
sleep 60
curl http://localhost:8000/health
make test-e2e
```

## ğŸ“ Support

- **Dokumentation**: Siehe `.cursorrules/rules/feature_testing.md`
- **Commands**: `make test-help`
- **CI/CD**: GitHub Actions in `.github/workflows/feature-testing.yml`
- **Validation**: `scripts/validate_feature_tests.py`

---

**Remember: Kein Feature ohne Tests! ğŸ§ªâœ¨**
