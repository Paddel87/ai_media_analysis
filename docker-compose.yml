# VPS-Optimierte Docker-Compose-Konfiguration
version: '3.8'

services:
  nginx:
    build:
      context: ./services/nginx
      dockerfile: Dockerfile
    ports:
      - "80:80"
      - "443:443"  # SSL-Support für Production
    depends_on:
      - data-persistence
      - redis
      - vector-db
    networks:
      - ai_network
    volumes:
      - ./config/nginx:/etc/nginx/conf.d:ro
      - ./logs/nginx:/var/log/nginx
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  data-persistence:
    build:
      context: .
      dockerfile: setup/Dockerfile.persistence
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
      - ./logs/persistence:/app/logs
    environment:
      - CONFIG_PATH=/app/config/persistence_config.json
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G  # Reduziert für VPS
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "python", "-c", "import os; assert os.path.exists('/app/data')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:7.0-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./logs/redis:/var/log/redis
    networks:
      - ai_network
    command: redis-server /usr/local/etc/redis/redis.conf --maxmemory 1gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1.5G  # Reduziert für VPS
        reservations:
          cpus: '0.5'
          memory: 512M
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # VPS-Services (CPU-Only)
  vector-db:
    build:
      context: ./services/vector_db
      dockerfile: Dockerfile
    ports:
      - "8002:8000"
    volumes:
      - vector_data:/app/data
      - ./logs/vector-db:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - FAISS_CPU_ONLY=1  # Explizit CPU-only
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Cloud AI-Ready Services (für Vast.ai Integration vorbereitet)
  pose_estimation:
    build:
      context: ./services/pose_estimation
      dockerfile: Dockerfile.cpu
    container_name: ai_pose_estimation
    ports:
      - "8003:8000"  # Exposed port für lokale Entwicklung
    volumes:
      - ./data/results:/app/results
      - ./logs/pose:/app/logs
      - ./config/pose:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
      - MODEL_TYPE=cpu
      - MAX_WORKERS=2
      - MEMORY_LIMIT=2G
      - PORT=8000
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ocr_detection:
    build:
      context: ./services/ocr_detection
      dockerfile: Dockerfile
    container_name: ai_ocr_detection
    volumes:
      - ./data/results:/app/results
      - ./logs/ocr:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  clip_nsfw:
    build:
      context: ./services/clip_nsfw
      dockerfile: Dockerfile
    container_name: ai_clip_nsfw
    volumes:
      - ./data/results:/app/results
      - ./logs/nsfw:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  face_reid:
    build:
      context: ./services/face_reid
      dockerfile: Dockerfile
    container_name: ai_face_reid
    environment:
      # OPTION A PRIMARY CONFIGURATION - CPU-FIRST
      - CPU_PRIMARY=true
      - CLOUD_OPTIONAL=false  # User-controllable
      - POWER_USER_MODE=true
      - LOCAL_FIRST=true

      # CPU-Optimized Configuration (PRIMARY)
      - CPU_MODEL=buffalo_s  # Balanced accuracy/speed
      - CPU_CORES=4
      - MULTI_THREADING=true
      - TARGET_LATENCY=800ms
      - MEMORY_LIMIT=3GB
      - BATCH_PROCESSING=true

      # Cloud Enhancement (OPTIONAL - Disabled by default)
      - CLOUD_ENHANCEMENT_AVAILABLE=true
      - CLOUD_MODEL=buffalo_l
      - CLOUD_COST_AWARENESS=true
      - CLOUD_USER_CONFIRMATION=required

      # Service Configuration
      - SERVICE_NAME=face_reid
      - SERVICE_PORT=8003
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=INFO

      # Power-User CPU Controls
      - CPU_PERFORMANCE_MONITORING=true
      - RESOURCE_OPTIMIZATION=true
      - LOCAL_MODEL_MANAGEMENT=true
      - THREAD_OPTIMIZATION=true
      - BENCHMARK_MODE=available
      - PROFILING_TOOLS=true

    ports:
      - "8003:8003"
    volumes:
      - ./data/models:/app/models
      - ./data/frames:/app/frames
      - ./logs/face_reid:/app/logs
      - ./config/hybrid_config.yml:/app/config/hybrid_config.yml
    networks:
      - ai_network
    depends_on:
      - redis
      - control
    restart: unless-stopped
    deploy:
      resources:
        # CPU-Primary Optimized Resources
        limits:
          cpus: '4.0'    # Full CPU power for Option A
          memory: 4G     # Adequate for CPU models
        reservations:
          cpus: '2.0'    # Guaranteed CPU
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Model loading time

  whisper_transcriber:
    build:
      context: ./services/whisper_transcriber
      dockerfile: Dockerfile
    container_name: ai_whisper_transcriber
    ports:
      - "8001:8000"
    volumes:
      - ./data/results:/app/results
      - ./services/whisper_transcriber/models:/app/models
      - ./logs/whisper:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - CLOUD_MODE=false
      - WHISPER_MODEL_SIZE=base  # Kleineres Modell für CPU
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '4'  # Whisper benötigt mehr CPU
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s  # Whisper braucht länger zum Starten
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Development UI Service
  streamlit-ui:
    build:
      context: ./services/ui
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./logs/ui:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - API_BASE_URL=http://nginx
    depends_on:
      - nginx
      - redis
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================================================
  # ITERATION 1: Management-Core Services (4 Services)
  # =============================================================================

  job_manager:
    build:
      context: ./services/job_manager
      dockerfile: Dockerfile
    container_name: ai_job_manager
    ports:
      - "8005:8000"
    volumes:
      - ./data/jobs:/app/jobs
      - ./logs/job_manager:/app/logs
      - ./config/job_manager:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - MAX_CONCURRENT_JOBS=10
      - JOB_TIMEOUT=3600
      - BATCH_ID=default_batch_iteration1
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  control:
    build:
      context: ./services/control
      dockerfile: Dockerfile
    container_name: ai_control
    ports:
      - "8006:8000"
    volumes:
      - ./data/control:/app/data
      - ./logs/control:/app/logs
      - ./config/control:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=2
      - SYSTEM_MODE=production
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  embedding_server:
    build:
      context: ./services/embedding_server
      dockerfile: Dockerfile
    container_name: ai_embedding_server
    ports:
      - "8007:8000"
    volumes:
      - ./data/embeddings:/app/embeddings
      - ./logs/embedding_server:/app/logs
      - ./config/embedding_server:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=3
      - VECTOR_DB_HOST=vector-db
      - VECTOR_DB_PORT=8000
      - MODEL_TYPE=cpu
      - BATCH_SIZE=32
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  llm_service:
    build:
      context: ./services/llm_service
      dockerfile: Dockerfile
    container_name: ai_llm_service
    ports:
      - "8008:8000"
    volumes:
      - ./data/llm:/app/data
      - ./logs/llm_service:/app/logs
      - ./config/llm_service:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=4
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - MODEL_PROVIDER=openai
      - MAX_TOKENS=4096
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================================================
  # UC-001 ENHANCED MANUAL ANALYSIS SERVICES
  # =============================================================================

  person_dossier:
    build:
      context: ./services/person_dossier
      dockerfile: Dockerfile
    container_name: ai_person_dossier
    ports:
      - "8009:8000"
    volumes:
      - ./data/dossiers:/app/data/dossiers:rw
      - ./data/corrections:/app/data/corrections:rw
      - ./logs/person_dossier:/app/logs
      - ./config/hybrid_config.yml:/app/config/hybrid_config.yml:ro
    environment:
      # OPTION A PRIMARY CONFIGURATION - CPU-FIRST
      - CPU_PRIMARY=true
      - CLOUD_OPTIONAL=false  # User-controllable
      - POWER_USER_MODE=true
      - LOCAL_FIRST=true

      # UC-001 CORE CONFIGURATION
      - UC001_ENABLED=true
      - DOSSIER_INTEGRATION=true
      - USER_CORRECTIONS=true
      - PERFORMANCE_MONITORING=true

      # SERVICE CONFIGURATION
      - SERVICE_NAME=person_dossier
      - SERVICE_PORT=8000
      - REDIS_URL=redis://redis:6379
      - REDIS_DB=5
      - LOG_LEVEL=INFO

      # CPU-OPTIMIZED SETTINGS
      - CPU_CORES=4
      - MULTI_THREADING=true
      - BATCH_PROCESSING=true
      - MEMORY_LIMIT=3GB

      # POWER-USER FEATURES
      - EXPORT_ENABLED=true
      - SEARCH_OPTIMIZATION=true
      - STATISTICS_DETAILED=true
      - FACE_SIMILARITY_THRESHOLD=0.85

      # CLOUD ENHANCEMENT (OPTIONAL)
      - CLOUD_ENHANCEMENT_AVAILABLE=true
      - CLOUD_COST_AWARENESS=true

    networks:
      - ai_network
    depends_on:
      - redis
      - control
    restart: unless-stopped
    deploy:
      resources:
        # UC-001 CORE SERVICE - Priority Resources
        limits:
          cpus: '4.0'    # Full CPU power for core UC-001 functionality
          memory: 4G     # Adequate for dossier management
        reservations:
          cpus: '2.0'    # Guaranteed CPU for responsiveness
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s  # Longer start for database initialization
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  video_context_analyzer:
    build:
      context: ./services/video_context_analyzer
      dockerfile: Dockerfile
    container_name: ai_video_context_analyzer
    ports:
      - "8010:8000"
    volumes:
      - ./data/video_analysis:/app/data/video_analysis:rw
      - ./data/context_results:/app/data/context_results:rw
      - ./data/llm_cache:/app/data/llm_cache:rw
      - ./data/videos:/app/data/videos:ro  # Read-only video access
      - ./logs/video_context_analyzer:/app/logs
      - ./config/hybrid_config.yml:/app/config/hybrid_config.yml:ro
    environment:
      # OPTION A PRIMARY CONFIGURATION - CPU-FIRST + LLM
      - CPU_PRIMARY=true
      - CLOUD_OPTIONAL=true   # LLM cloud enhancement available
      - POWER_USER_MODE=true
      - LOCAL_FIRST=true

      # UC-001 VIDEO CONTEXT ANALYZER CONFIGURATION
      - UC001_ENABLED=true
      - VIDEO_ANALYSIS=true
      - LLM_INTEGRATION=true
      - CONTEXT_GENERATION=true
      - MOVEMENT_TRACKING=true
      - EMOTION_ANALYSIS=true
      - AUDIO_PROCESSING=true

      # SERVICE CONFIGURATION
      - SERVICE_NAME=video_context_analyzer
      - SERVICE_PORT=8000
      - REDIS_URL=redis://redis:6379
      - REDIS_DB=6
      - LOG_LEVEL=INFO

      # CPU-FIRST VIDEO PROCESSING
      - VIDEO_CPU_THREADS=4
      - FRAME_BATCH_SIZE=16
      - ANALYSIS_QUEUE_SIZE=100
      - MEMORY_OPTIMIZATION=true

      # LLM OPTIMIZATION (Option A Primary)
      - LLM_LOCAL_FIRST=true
      - LLM_CLOUD_OPTIONAL=true
      - LLM_COST_AWARENESS=true
      - LLM_RESPONSE_CACHE=true

      # POWER-USER CONTROLS
      - EXPORT_ENABLED=true
      - DETAILED_LOGGING=true
      - PERFORMANCE_MONITORING=true
      - CONTEXT_QUALITY_THRESHOLD=0.8

      # CLOUD ENHANCEMENT API KEYS (OPTIONAL)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}

    networks:
      - ai_network
    depends_on:
      - redis
      - control
      - person_dossier
    restart: unless-stopped
    deploy:
      resources:
        # UC-001 VIDEO ANALYSIS SERVICE - High Performance Requirements
        limits:
          cpus: '6.0'    # High CPU for video processing + LLM
          memory: 6G     # Adequate for video frames + LLM processing
        reservations:
          cpus: '3.0'    # Guaranteed CPU for video processing
          memory: 3G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s  # Longer start for video processing initialization
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  clothing_analyzer:
    build:
      context: ./services/clothing_analyzer
      dockerfile: Dockerfile
    container_name: ai_clothing_analyzer
    ports:
      - "8011:8000"
    volumes:
      - ./data/clothing_analysis:/app/data/clothing_analysis:rw
      - ./data/classifications:/app/data/classifications:rw
      - ./data/category_cache:/app/data/category_cache:rw
      - ./data/frames:/app/data/frames:ro  # Read-only frame access
      - ./logs/clothing_analyzer:/app/logs
      - ./config/hybrid_config.yml:/app/config/hybrid_config.yml:ro
    environment:
      # OPTION A PRIMARY CONFIGURATION - CPU-FIRST + CLIP
      - CPU_PRIMARY=true
      - CLOUD_OPTIONAL=true   # Cloud enhancement available for better accuracy
      - POWER_USER_MODE=true
      - LOCAL_FIRST=true

      # UC-001 CLOTHING ANALYZER CONFIGURATION
      - UC001_ENABLED=true
      - CLOTHING_ANALYSIS=true
      - CATEGORY_COUNT=200
      - MATERIAL_DETECTION=true
      - STYLE_CLASSIFICATION=true
      - DOSSIER_INTEGRATION=true

      # SERVICE CONFIGURATION
      - SERVICE_NAME=clothing_analyzer
      - SERVICE_PORT=8000
      - REDIS_URL=redis://redis:6379
      - REDIS_DB=7
      - LOG_LEVEL=INFO

      # CPU-FIRST CLIP PROCESSING
      - CLIP_CPU_THREADS=4
      - CLIP_BATCH_SIZE=8
      - CLIP_MODEL=ViT-B/32
      - CLASSIFICATION_CACHE=true

      # FORCE CPU-ONLY FOR OPTION A PRIMARY
      - CUDA_VISIBLE_DEVICES=""
      - TORCH_DEVICE=cpu

      # POWER-USER FEATURES
      - EXPORT_ENABLED=true
      - DETAILED_CATEGORIZATION=true
      - CONFIDENCE_THRESHOLD=0.7
      - CATEGORY_HIERARCHY=true

      # CLOUD ENHANCEMENT (OPTIONAL)
      - CLOUD_CLASSIFICATION_AVAILABLE=true
      - CLOUD_COST_AWARENESS=true

      # API KEYS FOR CLOUD ENHANCEMENT (OPTIONAL)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}

    networks:
      - ai_network
    depends_on:
      - redis
      - control
      - person_dossier
    restart: unless-stopped
    deploy:
      resources:
        # UC-001 CLOTHING ANALYSIS SERVICE - CLIP Performance Requirements
        limits:
          cpus: '4.0'    # High CPU for CLIP processing
          memory: 4G     # Adequate for CLIP model + image processing
        reservations:
          cpus: '2.0'    # Guaranteed CPU for CLIP operations
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 150s  # Longer start for CLIP model loading
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  uc001_job_manager:
    build:
      context: ./services/job_manager
      dockerfile: Dockerfile.uc001
    container_name: ai_uc001_job_manager
    ports:
      - "8012:8012"
    volumes:
      - ./data/uc001:/app/data/uc001:rw
      - ./data/jobs:/app/data/jobs:rw
      - ./data/dossiers:/app/data/dossiers:rw
      - ./logs/uc001:/app/logs/uc001
      - ./config/uc001:/app/config/uc001:ro
    environment:
      # UC-001 ENHANCED MANUAL ANALYSIS - JOB MANAGER
      - UC001_ENABLED=true
      - UC001_RESEARCH_MODE=true
      - UC001_POWER_USER=true
      - UC001_MAX_CONCURRENT=5

      # PIPELINE ORCHESTRATION CONFIGURATION
      - PIPELINE_ORCHESTRATOR=true
      - SERVICE_COORDINATION=true
      - JOB_QUEUE_MANAGEMENT=true
      - QUALITY_MONITORING=true

      # SERVICE CONFIGURATION
      - SERVICE_NAME=uc001_job_manager
      - SERVICE_PORT=8012
      - REDIS_URL=redis://redis:6379
      - REDIS_DB=8
      - LOG_LEVEL=INFO

      # UC-001 SERVICE ENDPOINTS (Internal Docker Network)
      - PERSON_DOSSIER_URL=http://person_dossier:8009
      - VIDEO_CONTEXT_URL=http://video_context_analyzer:8010
      - CLOTHING_ANALYZER_URL=http://clothing_analyzer:8011

      # POWER-USER PIPELINE FEATURES
      - RESEARCH_PIPELINES=true
      - UNRESTRICTED_ANALYSIS=true
      - ADVANCED_MONITORING=true
      - EXPORT_CAPABILITIES=true

      # PERFORMANCE OPTIMIZATION
      - ASYNC_PROCESSING=true
      - PARALLEL_JOBS=true
      - REDIS_COORDINATION=true
      - REAL_TIME_STATUS=true

      # DEVELOPMENT & DEBUGGING
      - DEBUG_ENDPOINTS=true
      - QUEUE_INSPECTION=true
      - SERVICE_VALIDATION=true

    networks:
      - ai_network
    depends_on:
      - redis
      - control
      - person_dossier
      - video_context_analyzer
      - clothing_analyzer
    restart: unless-stopped
    deploy:
      resources:
        # UC-001 JOB MANAGER - PIPELINE ORCHESTRATION REQUIREMENTS
        limits:
          cpus: '3.0'    # CPU for job coordination and pipeline management
          memory: 3G     # Memory for job queues and service coordination
        reservations:
          cpus: '1.5'    # Guaranteed CPU for pipeline processing
          memory: 1.5G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8012/health/uc001"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 90s  # Allow time for service dependency validation
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ai_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16

volumes:
  redis_data:
    driver: local
  vector_data:
    driver: local
  data:
    driver: local
