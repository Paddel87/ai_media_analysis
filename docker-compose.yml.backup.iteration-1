# VPS-Optimierte Docker-Compose-Konfiguration
version: '3.8'

services:
  nginx:
    build:
      context: ./services/nginx
      dockerfile: Dockerfile
    ports:
      - "80:80"
      - "443:443"  # SSL-Support für Production
    depends_on:
      - data-persistence
      - redis
      - vector-db
    networks:
      - ai_network
    volumes:
      - ./config/nginx:/etc/nginx/conf.d:ro
      - ./logs/nginx:/var/log/nginx
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  data-persistence:
    build:
      context: .
      dockerfile: setup/Dockerfile.persistence
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
      - ./logs/persistence:/app/logs
    environment:
      - CONFIG_PATH=/app/config/persistence_config.json
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G  # Reduziert für VPS
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "python", "-c", "import os; assert os.path.exists('/app/data')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:7.0-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./logs/redis:/var/log/redis
    networks:
      - ai_network
    command: redis-server /usr/local/etc/redis/redis.conf --maxmemory 1gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1.5G  # Reduziert für VPS
        reservations:
          cpus: '0.5'
          memory: 512M
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # VPS-Services (CPU-Only)
  vector-db:
    build:
      context: ./services/vector_db
      dockerfile: Dockerfile
    ports:
      - "8002:8000"
    volumes:
      - vector_data:/app/data
      - ./logs/vector-db:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - FAISS_CPU_ONLY=1  # Explizit CPU-only
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Cloud AI-Ready Services (für Vast.ai Integration vorbereitet)
  pose_estimation:
    build:
      context: ./services/pose_estimation
      dockerfile: Dockerfile.cpu
    container_name: ai_pose_estimation
    ports:
      - "8003:8000"  # Exposed port für lokale Entwicklung
    volumes:
      - ./data/results:/app/results
      - ./logs/pose:/app/logs
      - ./config/pose:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
      - MODEL_TYPE=cpu
      - MAX_WORKERS=2
      - MEMORY_LIMIT=2G
      - PORT=8000
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ocr_detection:
    build:
      context: ./services/ocr_detection
      dockerfile: Dockerfile.cpu
    container_name: ai_ocr_detection
    volumes:
      - ./data/results:/app/results
      - ./logs/ocr:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  clip_nsfw:
    build:
      context: ./services/clip_nsfw
      dockerfile: Dockerfile.cpu
    container_name: ai_clip_nsfw
    volumes:
      - ./data/results:/app/results
      - ./logs/nsfw:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  face_reid:
    build:
      context: ./services/face_reid
      dockerfile: Dockerfile.cpu
    container_name: ai_face_reid
    volumes:
      - ./data/results:/app/results
      - ./logs/face:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CLOUD_MODE=false
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  whisper_transcriber:
    build:
      context: ./services/whisper_transcriber
      dockerfile: Dockerfile.cpu
    container_name: ai_whisper_transcriber
    ports:
      - "8001:8000"
    volumes:
      - ./data/results:/app/results
      - ./services/whisper_transcriber/models:/app/models
      - ./logs/whisper:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - CLOUD_MODE=false
      - WHISPER_MODEL_SIZE=base  # Kleineres Modell für CPU
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '4'  # Whisper benötigt mehr CPU
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s  # Whisper braucht länger zum Starten
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Development UI Service
  streamlit-ui:
    build:
      context: ./streamlit_ui
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./logs/ui:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - API_BASE_URL=http://nginx
    depends_on:
      - nginx
      - redis
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================================================
  # ITERATION 1: Management-Core Services (4 Services)
  # =============================================================================

  job_manager:
    build:
      context: ./services/job_manager
      dockerfile: Dockerfile
    container_name: ai_job_manager
    ports:
      - "8005:8000"
    volumes:
      - ./data/jobs:/app/jobs
      - ./logs/job_manager:/app/logs
      - ./config/job_manager:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - MAX_CONCURRENT_JOBS=10
      - JOB_TIMEOUT=3600
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  control:
    build:
      context: ./services/control
      dockerfile: Dockerfile
    container_name: ai_control
    ports:
      - "8006:8000"
    volumes:
      - ./data/control:/app/data
      - ./logs/control:/app/logs
      - ./config/control:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=2
      - SYSTEM_MODE=production
    depends_on:
      redis:
        condition: service_healthy
      job_manager:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  embedding_server:
    build:
      context: ./services/embedding_server
      dockerfile: Dockerfile
    container_name: ai_embedding_server
    ports:
      - "8007:8000"
    volumes:
      - ./data/embeddings:/app/embeddings
      - ./logs/embedding_server:/app/logs
      - ./config/embedding_server:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=3
      - VECTOR_DB_HOST=vector-db
      - VECTOR_DB_PORT=8000
      - MODEL_TYPE=cpu
      - BATCH_SIZE=32
    depends_on:
      redis:
        condition: service_healthy
      vector-db:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  llm_service:
    build:
      context: ./services/llm_service
      dockerfile: Dockerfile
    container_name: ai_llm_service
    ports:
      - "8008:8000"
    volumes:
      - ./data/llm:/app/data
      - ./logs/llm_service:/app/logs
      - ./config/llm_service:/app/config:ro
    environment:
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=4
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - MODEL_PROVIDER=openai
      - MAX_TOKENS=4096
    depends_on:
      redis:
        condition: service_healthy
      embedding_server:
        condition: service_healthy
    networks:
      - ai_network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ai_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16

volumes:
  redis_data:
    driver: local
  vector_data:
    driver: local
  data:
    driver: local
