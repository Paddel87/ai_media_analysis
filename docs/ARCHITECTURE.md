# AI Media Analysis System - Architektur-Dokumentation

## SystemÃ¼bersicht

Das AI Media Analysis System ist eine **verteilte, mikroservice-basierte Anwendung** zur Analyse von Medieninhalten mit verschiedenen KI-Modellen. Das System ist sowohl fÃ¼r **lokale Entwicklung** als auch **VPS-Deployment** optimiert.

### Hauptkomponenten

1. **Vision Pipeline Services**
   - **NSFW-Detection** (`clip_nsfw`, `nsfw_detection`)
   - **Pose Estimation** (`pose_estimation`)
   - **OCR Detection** (`ocr_detection`)
   - **Face Recognition** (`face_reid`)
   - **Restraint Detection** (`restraint_detection`)
   - **Object Review** (`object_review`)

2. **Audio-Analyse Services**
   - **Whisper Transcription** (`whisper_service`, `whisper_transcriber`)
   - **Audio Processing Pipeline**

3. **AI/ML Services**
   - **LLM Service** (`llm_service`, `llm_summarizer`)
   - **CLIP Service** (`clip_service`)
   - **Embedding Server** (`embedding_server`)

4. **Datenverarbeitung**
   - **Vector Database** (`vector_db`) - Qdrant/FAISS
   - **Redis Caching** - Sitzungsmanagement und Caching
   - **Job Manager** (`job_manager`) - Asynchrone Verarbeitung

5. **Infrastructure Services**
   - **Nginx** (`nginx`) - Load Balancer und Reverse Proxy
   - **Data Persistence** - Datenmanagement
   - **Guardrails** (`guardrails`) - Sicherheit und Validierung

6. **User Interface**
   - **Streamlit UI** (`ui`)
   - **Person Dossier** (`person_dossier`) - Personenverwaltung
   - **Control Interface** (`control`) - Systemkontrolle

## Technische Architektur

### Modernisierte Service-Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚    Load Balancer     â”‚    â”‚   AI Services     â”‚
â”‚   (Streamlit)   â”‚â—„â”€â”€â–ºâ”‚      (Nginx)         â”‚â—„â”€â”€â–ºâ”‚  Vision/Audio/LLM â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector DB     â”‚â—„â”€â”€â–ºâ”‚    Job Manager       â”‚â—„â”€â”€â–ºâ”‚  Redis Cache      â”‚
â”‚  (Qdrant/FAISS) â”‚    â”‚   (Orchestrierung)   â”‚    â”‚  (Session/Cache)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Data Persistence   â”‚
                       â”‚   (Files/Results)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependencies & Requirements-Architektur

```
requirements/
â”œâ”€â”€ base.txt           # ğŸ—ï¸  Core Production Dependencies
â”œâ”€â”€ development.txt    # ğŸ› ï¸  Development Tools (inherits base)
â”œâ”€â”€ testing.txt        # ğŸ§ª Test Framework (inherits dev)
â””â”€â”€ services/
    â”œâ”€â”€ llm.txt        # ğŸ¤– LLM & Vector DB Dependencies
    â”œâ”€â”€ vision.txt     # ğŸ‘ï¸  Computer Vision Dependencies
    â””â”€â”€ cloud.txt      # â˜ï¸  Cloud Storage Dependencies
```

### Komponenten-Details

#### Vision Pipeline Services
- **Technologien**: Python, OpenCV, PyTorch, MMPose, MMDetection
- **GPU-Support**: CUDA-beschleunigt mit CPU-Fallback
- **Batch-Verarbeitung**: Optimiert fÃ¼r Effizienz
- **Caching**: Redis-basiert fÃ¼r wiederholte Analysen
- **Services**: `pose_estimation`, `ocr_detection`, `face_reid`, `nsfw_detection`

#### Audio-Analyse Services
- **Technologien**: Whisper, torchaudio, librosa
- **Real-time**: Stream-Verarbeitung mÃ¶glich
- **Batch-Mode**: GroÃŸe Dateien effizient verarbeiten
- **Services**: `whisper_service`, `whisper_transcriber`

#### LLM & AI Services
- **Technologien**: LangChain, OpenAI, Transformers, ChromaDB
- **Vector Search**: Embedding-basierte Suche
- **Services**: `llm_service`, `llm_summarizer`, `embedding_server`, `clip_service`

#### Datenverarbeitung
- **Vector DB**: Qdrant fÃ¼r Production, FAISS fÃ¼r Development
- **Redis**: Caching, Session-Management, Job-Queues
- **Persistence**: File-basierte Datenhaltung mit Backup-Strategien

#### Frontend & Control
- **UI**: Streamlit mit Plotly-Visualisierungen
- **Management**: Personen-Dossiers, System-Kontrolle
- **Responsive**: Mobile-optimiert

## Deployment-Architektur

### Docker-basierte Mikroservices
```yaml
# Jeder Service in eigenem Container
services:
  - nginx           # Load Balancer
  - redis          # Cache & Sessions
  - vector-db      # Vector Search
  - pose_estimation # Computer Vision
  - llm_service    # Language Models
  - whisper_service # Audio Processing
  - ui             # Frontend
```

### VPS-Optimierungen
- **Memory-Limits**: Angepasst fÃ¼r 8GB+ VPS
- **CPU-Quotas**: Effiziente Ressourcenverteilung
- **Health-Checks**: Robust monitoring fÃ¼r alle Services
- **Restart-Policies**: `unless-stopped` fÃ¼r Ausfallsicherheit

#### VPS-spezifisches Resource Management
```yaml
# CPU-Optimierung fÃ¼r VPS
services:
  pose_estimation:
    deploy:
      resources:
        limits: { cpus: '2', memory: '2G' }
        reservations: { cpus: '1', memory: '1G' }
    environment:
      - CPU_ONLY=true
      - MAX_WORKERS=2
      - MEMORY_LIMIT=2G

# Memory-Optimierung
  redis:
    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru

# Storage-Optimierung
  vector-db:
    volumes:
      - vector_data:/app/data  # Persistent storage
    environment:
      - DISK_CACHE_SIZE=512MB
```

#### Cost-Efficiency Strategien
- **Dynamic Resource Allocation**: Services nur bei Bedarf skalieren
- **Resource Sharing**: Effiziente Nutzung von CPU/Memory zwischen Services
- **Auto-Cleanup**: Automatische Bereinigung von temporÃ¤ren Daten
- **Load-Based Scaling**: Services basierend auf tatsÃ¤chlicher Last starten/stoppen

#### VPS Auto-Scaling
```bash
# Scaling-Commands
make scale-up-ai      # AI Services bei hoher Last
make scale-down-ai    # AI Services bei niedriger Last
make vps-optimize     # Automatische Ressourcen-Optimierung
make cost-monitor     # Cost-Tracking und -Optimierung
```

### Ressourcen-Management
```yaml
# Beispiel Ressourcen-Allokation
pose_estimation:
  limits: { cpus: '2', memory: '2G' }
  reservations: { cpus: '1', memory: '1G' }
```

## Development-Architektur

### Modulare Requirements-Installation
```bash
# Service-spezifische Installation
make install-llm      # LLM Dependencies
make install-vision   # Vision Dependencies
make install-cloud    # Cloud Dependencies
make install-all      # VollstÃ¤ndige lokale Entwicklung
```

### Branching-Strategie
- **main**: Produktions-Code
- **experiment/**: Sichere Experimente
- **wip/**: Work-in-Progress Features
- **backup/**: Automatische Backups

### Code-QualitÃ¤t
- **Black + isort**: Automatische Formatierung
- **Flake8 + mypy**: Linting und Type-Checking
- **Pre-commit hooks**: Automatisierte QualitÃ¤tschecks
- **Pytest**: Umfassende Test-Suite

## Sicherheit & Compliance

### API-Sicherheit
- **Authentifizierung**: Token-basiert
- **Rate Limiting**: Nginx-basiert
- **Input Validierung**: Pydantic-Schemas
- **CORS**: Konfigurierbare Policies

### Daten-Sicherheit
- **VerschlÃ¼sselung**: TLS/SSL fÃ¼r alle Verbindungen
- **Cloud-Integration**: AWS, GCP, Azure support
- **Backup-Strategien**: Automatisierte Datensicherung
- **Guardrails**: Input/Output-Validierung

## Monitoring & Observability

### Health Monitoring
```bash
# Comprehensive Health Checks
make health-check      # Alle Services
make health-check-core # Core Services nur
make health-check-ai   # AI Services nur
```

### Logging-Architektur
- **Zentralisiertes Logging**: JSON-Format mit Rotation
- **Service-spezifische Logs**: Getrennte Log-Streams
- **Error-Tracking**: Strukturierte Fehlerbehandlung
- **Performance-Monitoring**: Metriken-basiert

### Development-Monitoring
```bash
make monitor          # Live Service Monitoring
make logs-all        # Alle Service Logs
make logs-core       # Core Service Logs
```

### VPS Performance & Cost Monitoring
```bash
# Resource Usage Monitoring
make vps-status       # VPS Resource Usage
make memory-monitor   # Memory Usage Tracking
make cpu-monitor      # CPU Usage Analysis
make storage-monitor  # Disk Usage Tracking

# Cost & Efficiency Monitoring
make cost-analysis    # Resource Cost Analysis
make efficiency-report # Resource Efficiency Report
make scaling-history  # Auto-Scaling History
make resource-alerts  # Proactive Resource Alerts
```

#### VPS-spezifische Metriken
- **CPU-Utilization**: Pro-Service CPU-Nutzung
- **Memory-Efficiency**: Memory-Leak Detection
- **Storage-Usage**: Disk Space Monitoring
- **Network-Traffic**: Bandwidth-Usage Tracking
- **Cost-Per-Service**: Service-basierte Kostenanalyse
- **Scaling-Efficiency**: Auto-Scaling Performance

## Skalierung & Performance

### Horizontale Skalierung
- **Load Balancing**: Nginx mit mehreren Service-Instanzen
- **Service-Replikation**: Docker Swarm ready
- **Database-Sharding**: Vector DB Clustering
- **Cache-Distribution**: Redis Cluster support

### Vertikale Skalierung
- **GPU-Ressourcen**: CUDA-optimiert mit CPU-Fallback
- **Memory-Optimierung**: Lazy Loading und Caching
- **CPU-Optimierung**: Async/Await Patterns
- **Storage-Optimierung**: Komprimierte Speicherung

### Performance-Testing
```bash
make benchmark       # Performance Benchmarks
make stress-test     # System-Stress-Tests
make load-test       # Load Testing
```

## Best Practices & Development

### Code-Architektur
- **Mikroservices**: Lose gekoppelt, stark kohÃ¤siv
- **Dependency Injection**: Konfigurierbare Services
- **Interface Segregation**: Klare API-Contracts
- **Event-Driven**: Asynchrone Kommunikation

### Testing-Strategie
```bash
make test-unit        # Unit Tests
make test-integration # Integration Tests
make test-e2e        # End-to-End Tests
make test-coverage   # Coverage Analysis
```

### Deployment-Pipeline
1. **Development**: Lokale Requirements-Installation
2. **Testing**: Automatisierte Test-Suite
3. **Staging**: VPS-Ã¤hnliche Umgebung
4. **Production**: Docker-basiertes Deployment

### Documentation
- **Code**: Inline-Dokumentation + Docstrings
- **API**: Automatisch generierte OpenAPI-Specs
- **Architecture**: Living Documentation (diese Datei)
- **Runbooks**: Operational Procedures

## Technologie-Stack

### Core Technologies
- **Python 3.11+**: Primary Development Language
- **FastAPI**: High-performance API Framework
- **Docker**: Containerization & Orchestration
- **Nginx**: Load Balancing & Reverse Proxy

### AI/ML Stack
- **PyTorch**: Deep Learning Framework
- **Transformers**: HuggingFace Models
- **OpenCV**: Computer Vision
- **Whisper**: Audio Transcription
- **LangChain**: LLM Orchestration

### Data & Storage
- **Redis**: Caching & Session Management
- **Qdrant/FAISS**: Vector Search
- **S3/GCS/Azure**: Cloud Storage Integration
- **SQLAlchemy**: Database ORM (wenn benÃ¶tigt)

### Development Tools
- **Black + isort**: Code Formatting
- **Flake8 + mypy**: Code Quality
- **Pytest**: Testing Framework
- **Pre-commit**: Git Hooks
